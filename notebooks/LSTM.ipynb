{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefb3bce-9a52-49a2-b2f3-99fc11a77201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 14:46:20,046 [INFO] Loading data from ../data/randhrs1992_2022v1.dta...\n",
      "/tmp/ipykernel_887379/4993709.py:189: FutureWarning: The StataReader.close() method is not part of the public API and will be removed in a future version without notice. Using StataReader as a context manager is the only supported method.\n",
      "  reader.close()\n",
      "/tmp/ipykernel_887379/4993709.py:196: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill', axis=1).fillna(0)\n",
      "2026-02-15 14:46:34,752 [INFO] Splitting data by ID (preventing leakage)...\n",
      "2026-02-15 14:46:34,785 [INFO] Fitting scaler on training data only...\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "2026-02-15 14:46:35,142 [INFO] Creating sequences...\n",
      "2026-02-15 14:46:35,532 [INFO] Class Imbalance: 1:5.00\n",
      "/tmp/ipykernel_887379/4993709.py:375: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
      "2026-02-15 14:46:37,045 [INFO] Starting Training...\n",
      "2026-02-15 14:47:31,852 [INFO] Epoch 1 | Loss: 1.0233 | Val AUC: 0.7192\n",
      "2026-02-15 14:48:27,052 [INFO] Epoch 2 | Loss: 0.9860 | Val AUC: 0.7637\n",
      "2026-02-15 14:49:21,512 [INFO] Epoch 3 | Loss: 0.9376 | Val AUC: 0.7725\n",
      "2026-02-15 14:50:17,027 [INFO] Epoch 4 | Loss: 0.9242 | Val AUC: 0.7785\n",
      "2026-02-15 14:51:13,011 [INFO] Epoch 5 | Loss: 0.9170 | Val AUC: 0.7807\n",
      "2026-02-15 14:52:08,779 [INFO] Epoch 6 | Loss: 0.9053 | Val AUC: 0.7876\n",
      "2026-02-15 14:53:05,298 [INFO] Epoch 7 | Loss: 0.8959 | Val AUC: 0.7901\n",
      "2026-02-15 14:54:00,185 [INFO] Epoch 8 | Loss: 0.8891 | Val AUC: 0.7918\n",
      "2026-02-15 14:54:55,111 [INFO] Epoch 9 | Loss: 0.8836 | Val AUC: 0.7933\n",
      "2026-02-15 14:55:50,004 [INFO] Epoch 10 | Loss: 0.8760 | Val AUC: 0.7917\n",
      "2026-02-15 14:55:50,004 [INFO] Best Validation AUC: 0.7933\n",
      "2026-02-15 14:55:50,005 [INFO] Running Final Evaluation on Best Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL PROCESS RESULTS\n",
      "==================================================\n",
      "ROC AUC:    0.7933\n",
      "PR AUC:     0.3674\n",
      "F2 SCORE:   0.6336 (at threshold 0.38)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "RAND HRS: Bi-LSTM + Attention (No Leakage) + Full Evaluation\n",
    "=============================================================================\n",
    "Summary:\n",
    "- Input: Rolling window of 4 waves (Risk Factors Only).\n",
    "- Target: Health Decline in next wave (derived from excluded variables).\n",
    "- Model: Bidirectional LSTM with Temporal Attention.\n",
    "- Evaluation: ROC, PR, F2-Score optimization, Confusion Matrix.\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, \n",
    "    roc_curve, fbeta_score, confusion_matrix, ConfusionMatrixDisplay, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. CONFIGURATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "CFG = {\n",
    "    \"dta_path\": \"../data/randhrs1992_2022v1.dta\",\n",
    "    \"output_dir\": \"./outputs_lstm_final\",\n",
    "    \"seq_len\": 4,             \n",
    "    \"batch_size\": 128,        \n",
    "    \"epochs\": 10,             \n",
    "    \"lr\": 5e-4,               \n",
    "    \"hidden_dim\": 128,        \n",
    "    \"lstm_layers\": 3,         \n",
    "    \"dropout\": 0.4,           \n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CFG[\"seed\"])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. FEATURE DEFINITION (STRICT NO LEAKAGE)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "MAX_WAVES = 15\n",
    "def get_wave_cols(prefix, suffix, max_w=MAX_WAVES):\n",
    "    return [f\"{prefix}{w}{suffix}\" for w in range(1, max_w + 1)]\n",
    "\n",
    "# INPUT FEATURES (X) - Risk Factors Only\n",
    "INPUT_FEATURES = {\n",
    "    \"bmi\":      get_wave_cols(\"r\", \"bmi\"),\n",
    "    \"cesd\":     get_wave_cols(\"r\", \"cesd\"),      # Depression\n",
    "    \"adl\":      get_wave_cols(\"r\", \"adla\"),\n",
    "    \"iadl\":     get_wave_cols(\"r\", \"iadlza\"),\n",
    "    \"vig_act\":  get_wave_cols(\"r\", \"vgactx\"),    # Exercise\n",
    "    \"wealth\":   get_wave_cols(\"h\", \"atotb\"),     # Socioeconomic\n",
    "    \"income\":   get_wave_cols(\"h\", \"itot\"),      # Total Household Income\n",
    "    \"work\":     get_wave_cols(\"r\", \"work\"),\n",
    "    \"livsib\":   get_wave_cols(\"r\", \"livsib\"),\n",
    "    \"weight\":   get_wave_cols(\"r\", \"weight\"),\n",
    "    \"drinkr\":   get_wave_cols(\"r\", \"drinkr\"),\n",
    "    \"evbrn\":    get_wave_cols(\"r\", \"evbrn\"),\n",
    "    \"smoken\":   get_wave_cols(\"r\", \"smoken\"),\n",
    "    \"smokev\":   get_wave_cols(\"r\", \"smokev\"),\n",
    "    \"urbrur\":   get_wave_cols(\"r\", \"urbrur\"),\n",
    "    \"height\":   get_wave_cols(\"r\", \"height\"),\n",
    "}\n",
    "\n",
    "# TARGET SOURCES (y) - Excluded from Input\n",
    "TARGET_SOURCE_FEATURES = {\n",
    "    \"srh\":      get_wave_cols(\"r\", \"shlt\"),    \n",
    "    \"cond\":     get_wave_cols(\"r\", \"conde\"),   \n",
    "}\n",
    "\n",
    "STATIC_COLS = [\"ragender\", \"raracem\", \"rahispan\", \"raedyrs\", \"rabyear\"]\n",
    "ID_COL = \"hhidpn\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. DATASET CLASS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class SimpleDS(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. PREPROCESSING (LEAKAGE FIX)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class HRSPreprocessor:\n",
    "    \"\"\"Handles scaling and clipping fitted ONLY on training data.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.limits = {}\n",
    "        self.static_scaler = RobustScaler()\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Learns stats from Training Set Only.\"\"\"\n",
    "        # Fit Dynamic Features\n",
    "        for feat_name, cols in INPUT_FEATURES.items():\n",
    "            vals = df[cols].values.flatten().reshape(-1, 1)\n",
    "            # Clip logic\n",
    "            lim = np.percentile(vals, 99)\n",
    "            self.limits[feat_name] = lim\n",
    "            if lim > 0:\n",
    "                vals = np.clip(vals, -lim, lim)\n",
    "            \n",
    "            # Scaler logic\n",
    "            scl = RobustScaler()\n",
    "            scl.fit(vals)\n",
    "            self.scalers[feat_name] = scl\n",
    "\n",
    "        # Fit Static Features\n",
    "        static_vals = df[STATIC_COLS].values\n",
    "        self.static_scaler.fit(static_vals)\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Applies learned stats to Train or Test set.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted on training data first.\")\n",
    "        \n",
    "        df_out = df.copy()\n",
    "        \n",
    "        # Transform Dynamic\n",
    "        for feat_name, cols in INPUT_FEATURES.items():\n",
    "            vals = df_out[cols].values\n",
    "            orig_shape = vals.shape\n",
    "            vals = vals.flatten().reshape(-1, 1)\n",
    "            \n",
    "            # Apply saved clip\n",
    "            lim = self.limits[feat_name]\n",
    "            if lim > 0:\n",
    "                vals = np.clip(vals, -lim, lim)\n",
    "            \n",
    "            # Apply saved scaler\n",
    "            vals = self.scalers[feat_name].transform(vals)\n",
    "            df_out[cols] = vals.reshape(orig_shape)\n",
    "\n",
    "        # Transform Static\n",
    "        df_out[STATIC_COLS] = self.static_scaler.transform(df_out[STATIC_COLS])\n",
    "        \n",
    "        return df_out\n",
    "\n",
    "def load_data_raw(path):\n",
    "    \"\"\"Loads data and performs basic imputation (No Scaling).\"\"\"\n",
    "    log.info(f\"Loading data from {path}...\")\n",
    "    \n",
    "    input_cols = [c for cols in INPUT_FEATURES.values() for c in cols]\n",
    "    target_cols = [c for cols in TARGET_SOURCE_FEATURES.values() for c in cols]\n",
    "    all_needed = [ID_COL] + STATIC_COLS + input_cols + target_cols\n",
    "    \n",
    "    try:\n",
    "        df_cols = pd.read_stata(path, nrows=0).columns.tolist()\n",
    "    except:\n",
    "        reader = pd.read_stata(path, iterator=True)\n",
    "        df_cols = list(reader.variable_labels().keys())\n",
    "        reader.close()\n",
    "        \n",
    "    valid_cols = [c for c in all_needed if c in df_cols]\n",
    "    df = pd.read_stata(path, columns=valid_cols, convert_categoricals=False)\n",
    "    df = df.reindex(columns=all_needed)\n",
    "    \n",
    "    # Impute missing values with 0 or ffill (Basic cleanup)\n",
    "    df = df.fillna(method='ffill', axis=1).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sequences(df):\n",
    "    \"\"\"Converts Wide DF to Sequences using pre-calculated targets.\"\"\"\n",
    "    # Extract Target Sources (Raw values, unscaled)\n",
    "    raw_srh = df[TARGET_SOURCE_FEATURES[\"srh\"]].values\n",
    "    raw_cond = df[TARGET_SOURCE_FEATURES[\"cond\"]].values\n",
    "    \n",
    "    sequences, targets, groups = [], [], []\n",
    "    valid_starts = range(1, MAX_WAVES - CFG[\"seq_len\"] + 1)\n",
    "    \n",
    "    id_arr = df[ID_COL].values\n",
    "    static_arr = df[STATIC_COLS].values\n",
    "    temp_arrays = {k: df[v].values for k, v in INPUT_FEATURES.items()}\n",
    "    \n",
    "    for start_w in valid_starts:\n",
    "        input_idxs = [w-1 for w in range(start_w, start_w + CFG[\"seq_len\"])]\n",
    "        last_in_idx = input_idxs[-1]\n",
    "        target_idx  = last_in_idx + 1\n",
    "        \n",
    "        # Build Input X\n",
    "        batch_inputs = []\n",
    "        for t in input_idxs:\n",
    "            step_feats = [temp_arrays[k][:, t:t+1] for k in INPUT_FEATURES.keys()]\n",
    "            batch_inputs.append(np.concatenate(step_feats, axis=1))\n",
    "        X_time = np.stack(batch_inputs, axis=1)\n",
    "        \n",
    "        # Add Static\n",
    "        X_static = np.tile(static_arr[:, np.newaxis, :], (1, CFG[\"seq_len\"], 1))\n",
    "        X_full = np.concatenate([X_time, X_static], axis=2)\n",
    "        \n",
    "        # Calculate Target (Change in Raw Srh or Cond)\n",
    "        # Note: raw_srh/cond indices align with wave numbers (0 = wave 1)\n",
    "        srh_diff = raw_srh[:, target_idx] - raw_srh[:, last_in_idx]\n",
    "        cond_diff = raw_cond[:, target_idx] - raw_cond[:, last_in_idx]\n",
    "        y = ((srh_diff >= 1.0) | (cond_diff >= 1.0)).astype(int)\n",
    "        \n",
    "        sequences.append(X_full)\n",
    "        targets.append(y)\n",
    "        groups.append(id_arr)\n",
    "        \n",
    "    return np.concatenate(sequences, axis=0), np.concatenate(targets, axis=0), np.concatenate(groups, axis=0)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. MODEL\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, lstm_output):\n",
    "        attn_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class BiLSTM_Attn(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, CFG[\"hidden_dim\"], CFG[\"lstm_layers\"], \n",
    "                            batch_first=True, bidirectional=True, dropout=CFG[\"dropout\"])\n",
    "        self.attn = Attention(CFG[\"hidden_dim\"])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(CFG[\"hidden_dim\"] * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CFG[\"dropout\"]),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        context, _ = self.attn(out)\n",
    "        return self.fc(context)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. EVALUATION FUNCTIONS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_performance(y_true, y_probs, save_dir):\n",
    "    \"\"\"Generates plots and calculates metrics.\"\"\"\n",
    "    \n",
    "    # 1. Basic Curves\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    pr_auc = average_precision_score(y_true, y_probs)\n",
    "    \n",
    "    # 2. Optimal Threshold for F2 Score\n",
    "    best_f2 = 0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    for t in np.arange(0.01, 1.0, 0.01):\n",
    "        y_pred_t = (y_probs >= t).astype(int)\n",
    "        score = fbeta_score(y_true, y_pred_t, beta=2)\n",
    "        if score > best_f2:\n",
    "            best_f2 = score\n",
    "            best_thresh = t\n",
    "            \n",
    "    # 3. Final Predictions using Best Threshold\n",
    "    y_pred_final = (y_probs >= best_thresh).astype(int)\n",
    "    \n",
    "    # 4. Save Plots\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{save_dir}/roc_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(f\"{save_dir}/pr_curve.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred_final)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Stable\", \"Decline\"])\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "    plt.title(f'Confusion Matrix (Threshold={best_thresh:.2f})')\n",
    "    plt.savefig(f\"{save_dir}/confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return best_thresh, roc_auc, pr_auc, best_f2\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. TRAINING PIPELINE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def train():\n",
    "    # 1. Load Raw Data\n",
    "    df_raw = load_data_raw(CFG[\"dta_path\"])\n",
    "    \n",
    "    # 2. Split IDs (Train/Val) BEFORE Scaling\n",
    "    log.info(\"Splitting data by ID (preventing leakage)...\")\n",
    "    splitter = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=CFG[\"seed\"])\n",
    "    train_idx, val_idx = next(splitter.split(df_raw, groups=df_raw[ID_COL]))\n",
    "    \n",
    "    df_train = df_raw.iloc[train_idx].copy()\n",
    "    df_val = df_raw.iloc[val_idx].copy()\n",
    "    \n",
    "    # 3. Fit Scaler on TRAIN, Transform TRAIN and VAL\n",
    "    log.info(\"Fitting scaler on training data only...\")\n",
    "    preprocessor = HRSPreprocessor()\n",
    "    preprocessor.fit(df_train)\n",
    "    \n",
    "    df_train_scaled = preprocessor.transform(df_train)\n",
    "    df_val_scaled = preprocessor.transform(df_val)\n",
    "    \n",
    "    # 4. Create Sequences\n",
    "    log.info(\"Creating sequences...\")\n",
    "    X_tr, y_tr, _ = create_sequences(df_train_scaled)\n",
    "    X_val, y_val, _ = create_sequences(df_val_scaled)\n",
    "    \n",
    "    # Convert to Tensor\n",
    "    X_tr = torch.FloatTensor(X_tr)\n",
    "    y_tr = torch.FloatTensor(y_tr)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(y_val)\n",
    "    \n",
    "    train_ds = SimpleDS(X_tr, y_tr)\n",
    "    val_ds = SimpleDS(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CFG[\"batch_size\"])\n",
    "    \n",
    "    # Calculate Class Weights\n",
    "    pos_weight = (y_tr == 0).sum() / (y_tr == 1).sum()\n",
    "    log.info(f\"Class Imbalance: 1:{pos_weight:.2f}\")\n",
    "    \n",
    "    # Model Setup\n",
    "    model = BiLSTM_Attn(input_dim=X_tr.shape[2]).to(CFG[\"device\"])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"])\n",
    "    \n",
    "    best_auc = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    log.info(\"Starting Training...\")\n",
    "    for epoch in range(CFG[\"epochs\"]):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(CFG[\"device\"]), yb.to(CFG[\"device\"])\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb).squeeze()\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        model.eval()\n",
    "        probs, true_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(CFG[\"device\"])\n",
    "                probs.extend(torch.sigmoid(model(xb).squeeze()).cpu().numpy())\n",
    "                true_y.extend(yb.numpy())\n",
    "        \n",
    "        auc = roc_auc_score(true_y, probs)\n",
    "        log.info(f\"Epoch {epoch+1} | Loss: {np.mean(losses):.4f} | Val AUC: {auc:.4f}\")\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            torch.save(model.state_dict(), f\"{CFG['output_dir']}/best_model.pth\")\n",
    "            \n",
    "    log.info(f\"Best Validation AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    # FINAL EVALUATION & METRICS\n",
    "    # ─────────────────────────────────────────────────────────────────────────\n",
    "    log.info(\"Running Final Evaluation on Best Model...\")\n",
    "    model.load_state_dict(torch.load(f\"{CFG['output_dir']}/best_model.pth\"))\n",
    "    model.eval()\n",
    "    \n",
    "    final_probs = []\n",
    "    final_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(CFG[\"device\"])\n",
    "            final_probs.extend(torch.sigmoid(model(xb).squeeze()).cpu().numpy())\n",
    "            final_true.extend(yb.numpy())\n",
    "            \n",
    "    final_true = np.array(final_true)\n",
    "    final_probs = np.array(final_probs)\n",
    "    \n",
    "    # Get Metrics\n",
    "    best_thresh, roc_auc, pr_auc, f2_score = evaluate_performance(final_true, final_probs, CFG[\"output_dir\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL PROCESS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ROC AUC:    {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC:     {pr_auc:.4f}\")\n",
    "    print(f\"F2 SCORE:   {f2_score:.4f} (at threshold {best_thresh:.2f})\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b86cdf-bf0f-40e9-a8c7-518e6fd49983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
